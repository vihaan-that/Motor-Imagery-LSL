{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mne\n",
        "import os\n",
        "import numpy as np\n",
        "import warnings\n",
        "from mne.preprocessing import ICA\n",
        "import mne_icalabel\n",
        "import matplotlib.pyplot as plt\n",
        "from mne.channels import make_dig_montage\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from pyriemann.estimation import Covariances\n",
        "from pyriemann.tangentspace import TangentSpace\n",
        "from pyriemann.utils.mean import mean_riemann\n",
        "from mne.decoding import CSP\n",
        "from scipy.signal import welch\n",
        "import pandas as pd\n",
        "import gc\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (accuracy_score, confusion_matrix,\n",
        "                            ConfusionMatrixDisplay, RocCurveDisplay,\n",
        "                            PrecisionRecallDisplay, log_loss)\n",
        "\n",
        "# Suppress warnings and set MNE logging to minimal\n",
        "warnings.filterwarnings('ignore')\n",
        "mne.set_log_level('WARNING')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U0ZOpPnFpUsK",
        "outputId": "2d568cfe-aac4-4d9e-a0e1-bc1a3d44962b"
      },
      "outputs": [],
      "source": [
        "# def create_eegmmidb_montage():\n",
        "#     \"\"\"\n",
        "#     Create a custom montage for the 14 selected channels based on standard_1020.\n",
        "#     \"\"\"\n",
        "#     emotiv_channels = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
        "\n",
        "#     standard_montage = mne.channels.make_standard_montage('standard_1020')\n",
        "\n",
        "#     ch_pos = {}0\n",
        "#     for ch in emotiv_channels:\n",
        "#         try:\n",
        "#             pos = standard_montage.dig[standard_montage.ch_names.index(ch)]['r']\n",
        "#             ch_pos[ch] = pos\n",
        "#         except ValueError:\n",
        "#             print(f\"Warning: Position for channel {ch} not found in standard_1020. Skipping.\")\n",
        "\n",
        "#     custom_montage = make_dig_montage(ch_pos=ch_pos, coord_frame='head')\n",
        "#     return custom_montage\n",
        "\n",
        "# def preprocess_data(subject, subject_folder, output_folder, epoch_counts):\n",
        "#     \"\"\"\n",
        "#     Preprocess EEG data from EEGMMIDB for left_fist (T1, runs 4,8,12) and both_feet (T2, runs 6,10,14).\n",
        "#     Track epoch counts for left_fist and both_feet tasks.\n",
        "#     \"\"\"\n",
        "#     all_epochs_list = []\n",
        "\n",
        "#     # Define runs\n",
        "#     left_fist_runs = [4, 8, 12]  # T1: left_fist imagined\n",
        "#     both_feet_runs = [6, 10, 14]  # T2: both_feet imagined\n",
        "#     all_runs = left_fist_runs + both_feet_runs\n",
        "\n",
        "#     for run in all_runs:\n",
        "#         edf_file = os.path.join(subject_folder, f'{subject}R{run:02d}.edf')\n",
        "#         if not os.path.exists(edf_file):\n",
        "#             print(f\"File {edf_file} not found, skipping.\")\n",
        "#             continue\n",
        "\n",
        "#         try:\n",
        "#             # 1. Data Loading and Channel Setup\n",
        "#             raw = mne.io.read_raw_edf(edf_file, preload=True, verbose=False)\n",
        "#             print(f\"Raw channel names before renaming in {edf_file}: {raw.ch_names}\")\n",
        "#             raw.rename_channels({ch: ch.rstrip('.').upper() for ch in raw.ch_names})\n",
        "#             print(f\"Raw channel names after renaming in {edf_file}: {raw.ch_names}\")\n",
        "\n",
        "#             # Select only the specified 14 channels\n",
        "#             emotiv_channels = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1',\n",
        "#                                'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
        "#             raw.pick_channels(emotiv_channels, ordered=True)\n",
        "\n",
        "#             raw.set_eeg_reference('average', verbose=False)\n",
        "#             montage = create_eegmmidb_montage()\n",
        "#             raw.set_montage(montage, on_missing='ignore')\n",
        "#             raw.filter(4, 40, method='fir', phase='zero-double', verbose=False)\n",
        "\n",
        "#             # 2. ICA Processing\n",
        "#             n_components = min(len(raw.ch_names), 14)  # Adjusted for 14 channels\n",
        "#             ica = ICA(n_components=n_components, method='picard', max_iter=1000, random_state=42)\n",
        "#             ica.fit(raw, reject_by_annotation=True, verbose=False)\n",
        "#             ic_labels = mne_icalabel.label_components(raw, ica, method='iclabel')\n",
        "#             ica.exclude = [i for i, label in enumerate(ic_labels['labels'])\n",
        "#                           if label in ('eye blink', 'muscle artifact', 'heart beat', 'line noise')]\n",
        "#             raw = ica.apply(raw, verbose=False)\n",
        "\n",
        "#             # 3. Event Extraction and Epoch Creation\n",
        "#             events = []\n",
        "#             annotations = raw.annotations.description\n",
        "#             unique_annotations = set(annotations)\n",
        "#             print(f\"Annotations in {edf_file}: {unique_annotations}\")\n",
        "\n",
        "#             # Count relevant events only\n",
        "#             if run in left_fist_runs:\n",
        "#                 t_count = sum(1 for ann in annotations if ann == 'T1')\n",
        "#                 print(f\"Event counts in {edf_file}: T1 (left_fist) = {t_count}\")\n",
        "#                 event_id = {'left_fist': 0}\n",
        "#             else:  # both_feet_runs\n",
        "#                 t_count = sum(1 for ann in annotations if ann == 'T2')\n",
        "#                 print(f\"Event counts in {edf_file}: T2 (both_feet) = {t_count}\")\n",
        "#                 event_id = {'both_feet': 1}\n",
        "\n",
        "#             for ann in raw.annotations:\n",
        "#                 desc = ann['description']\n",
        "#                 event_code = None\n",
        "#                 # Only process T1 for left_fist_runs, T2 for both_feet_runs\n",
        "#                 if run in left_fist_runs and desc == 'T1':\n",
        "#                     event_code = 0  # left_fist\n",
        "#                 elif run in both_feet_runs and desc == 'T2':\n",
        "#                     event_code = 1  # both_feet\n",
        "\n",
        "#                 if event_code is not None:\n",
        "#                     onset_sample = int(ann['onset'] * raw.info['sfreq'])\n",
        "#                     events.append([onset_sample, 0, event_code])\n",
        "\n",
        "#             if len(events) < 4:\n",
        "#                 print(f\"Skipping {edf_file} - insufficient events ({len(events)})\")\n",
        "#                 continue\n",
        "\n",
        "#             epochs = mne.Epochs(\n",
        "#                 raw,\n",
        "#                 np.array(events),\n",
        "#                 event_id=event_id,\n",
        "#                 tmin=-0.5,\n",
        "#                 tmax=3.5,\n",
        "#                 baseline=(-0.5, 0),\n",
        "#                 preload=True,\n",
        "#                 reject=dict(eeg=200e-6),\n",
        "#                 verbose=False\n",
        "#             )\n",
        "\n",
        "#             epochs = epochs.crop(tmin=0, tmax=3).resample(128)\n",
        "#             epochs.baseline = None\n",
        "#             all_epochs_list.append(epochs)\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing {edf_file}: {str(e)}\")\n",
        "#             continue\n",
        "\n",
        "#     if all_epochs_list:\n",
        "#         try:\n",
        "#             # Ensure consistent event_id when combining epochs\n",
        "#             for epochs in all_epochs_list:\n",
        "#                 if epochs.event_id == {'left_fist': 0}:\n",
        "#                     epochs.event_id = {'left_fist': 0, 'both_feet': 1}\n",
        "#                     epochs.events[epochs.events[:, 2] == 0, 2] = 0\n",
        "#                 elif epochs.event_id == {'both_feet': 1}:\n",
        "#                     epochs.event_id = {'left_fist': 0, 'both_feet': 1}\n",
        "#                     epochs.events[epochs.events[:, 2] == 1, 2] = 1\n",
        "\n",
        "#             combined_epochs = mne.concatenate_epochs(all_epochs_list)\n",
        "#             epochs_fname = os.path.join(output_folder, f'{subject}_eegmmidb_epochs-epo.fif')\n",
        "#             combined_epochs.save(epochs_fname, overwrite=True, verbose=False)\n",
        "\n",
        "#             left_count = sum(combined_epochs.events[:, 2] == 0)\n",
        "#             feet_count = sum(combined_epochs.events[:, 2] == 1)\n",
        "#             epoch_counts[subject] = {'left_fist': left_count, 'both_feet': feet_count}\n",
        "\n",
        "#             print(f\"\\nProcessed {len(combined_epochs)} epochs with {len(combined_epochs.info['ch_names'])} channels:\")\n",
        "#             print(combined_epochs.info['ch_names'])\n",
        "#             print(f\"Class balance - Left Fist: {left_count}, Both Feet: {feet_count}\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error combining epochs for {subject}: {str(e)}\")\n",
        "\n",
        "# def plot_epoch_counts(epoch_counts):\n",
        "#     \"\"\"\n",
        "#     Plot the number of epochs for each task per subject as a grouped bar plot.\n",
        "#     \"\"\"\n",
        "#     if not epoch_counts:\n",
        "#         print(\"No epoch counts to plot.\")\n",
        "#         return\n",
        "\n",
        "#     subjects = list(epoch_counts.keys())\n",
        "#     fists_counts = [epoch_counts[subj]['left_fist'] for subj in subjects]\n",
        "#     feet_counts = [epoch_counts[subj]['both_feet'] for subj in subjects]\n",
        "\n",
        "#     bar_width = 0.35\n",
        "#     index = np.arange(len(subjects))\n",
        "\n",
        "#     plt.figure(figsize=(12, 6))\n",
        "#     plt.bar(index, fists_counts, bar_width, label='Left Fist', color='skyblue')\n",
        "#     plt.bar(index + bar_width, feet_counts, bar_width, label='Both Feet', color='lightcoral')\n",
        "\n",
        "#     plt.xlabel('Subject')\n",
        "#     plt.ylabel('Number of Epochs')\n",
        "#     plt.title('Number of Epochs per Task for Each Subject')\n",
        "#     plt.xticks(index + bar_width / 2, subjects, rotation=45)\n",
        "#     plt.legend()\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig('epoch_counts.png')\n",
        "#     plt.close()\n",
        "#     print(\"Epoch counts plot saved to 'epoch_counts.png'\")\n",
        "\n",
        "# def main():\n",
        "#     \"\"\"Main processing routine\"\"\"\n",
        "#     data_dir = 'files'\n",
        "#     output_folder = 'processed_eegmmidb_data'\n",
        "#     os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "#     epoch_counts = {}\n",
        "#     subjects = [d for d in os.listdir(data_dir)\n",
        "#                 if os.path.isdir(os.path.join(data_dir, d))]\n",
        "\n",
        "#     for subject in subjects:\n",
        "#         print(f\"\\n{'='*40}\\nProcessing {subject}\")\n",
        "#         preprocess_data(subject, os.path.join(data_dir, subject), output_folder, epoch_counts)\n",
        "\n",
        "#     print(\"\\nProcessing completed. EEGMMIDB data ready for analysis.\")\n",
        "#     plot_epoch_counts(epoch_counts)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIYpwHgVqtCT",
        "outputId": "d1dc6667-f30d-4ff5-ccb3-6f953a31737b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded S001_eegmmidb_epochs-epo.fif: 47 epochs\n",
            "Loaded S002_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S003_eegmmidb_epochs-epo.fif: 47 epochs\n",
            "Loaded S004_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S005_eegmmidb_epochs-epo.fif: 43 epochs\n",
            "Loaded S006_eegmmidb_epochs-epo.fif: 47 epochs\n",
            "Loaded S007_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S008_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S009_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S010_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S011_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S012_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S013_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S014_eegmmidb_epochs-epo.fif: 43 epochs\n",
            "Loaded S015_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S016_eegmmidb_epochs-epo.fif: 43 epochs\n",
            "Loaded S017_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S018_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S019_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S020_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S021_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S022_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S023_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S024_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S025_eegmmidb_epochs-epo.fif: 43 epochs\n",
            "Loaded S026_eegmmidb_epochs-epo.fif: 48 epochs\n",
            "Loaded S027_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S028_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S029_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S030_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S031_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S032_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S033_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S034_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S035_eegmmidb_epochs-epo.fif: 43 epochs\n",
            "Loaded S036_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S037_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S038_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S039_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S040_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S041_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S042_eegmmidb_epochs-epo.fif: 43 epochs\n",
            "Loaded S043_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S044_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S045_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S046_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S047_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S048_eegmmidb_epochs-epo.fif: 43 epochs\n",
            "Loaded S049_eegmmidb_epochs-epo.fif: 47 epochs\n",
            "Loaded S050_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S051_eegmmidb_epochs-epo.fif: 48 epochs\n",
            "Loaded S052_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S053_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S054_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S055_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S056_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S057_eegmmidb_epochs-epo.fif: 47 epochs\n",
            "Loaded S058_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S059_eegmmidb_epochs-epo.fif: 47 epochs\n",
            "Loaded S060_eegmmidb_epochs-epo.fif: 47 epochs\n",
            "Loaded S061_eegmmidb_epochs-epo.fif: 43 epochs\n",
            "Loaded S062_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S063_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S064_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S065_eegmmidb_epochs-epo.fif: 47 epochs\n",
            "Loaded S066_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S067_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S068_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S069_eegmmidb_epochs-epo.fif: 47 epochs\n",
            "Loaded S070_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S071_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S072_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S073_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S074_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S075_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S076_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S077_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S078_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S079_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S080_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S081_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S082_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S083_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S084_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S085_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S086_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S087_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S088_eegmmidb_epochs-epo.fif: 57 epochs\n",
            "Loaded S089_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S090_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S091_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S092_eegmmidb_epochs-epo.fif: 54 epochs\n",
            "Loaded S093_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S094_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S095_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S096_eegmmidb_epochs-epo.fif: 47 epochs\n",
            "Loaded S097_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S098_eegmmidb_epochs-epo.fif: 47 epochs\n",
            "Loaded S099_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S100_eegmmidb_epochs-epo.fif: 36 epochs\n",
            "Loaded S101_eegmmidb_epochs-epo.fif: 43 epochs\n",
            "Loaded S102_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S103_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S104_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S105_eegmmidb_epochs-epo.fif: 44 epochs\n",
            "Loaded S106_eegmmidb_epochs-epo.fif: 46 epochs\n",
            "Loaded S107_eegmmidb_epochs-epo.fif: 47 epochs\n",
            "Loaded S108_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded S109_eegmmidb_epochs-epo.fif: 45 epochs\n",
            "Loaded 4932 epochs, 14 channels, 384 time points\n",
            "Class distribution - left_fist: 2478, both_feet: 2454\n",
            "Extracting CSP features...\n",
            "EnsureFloat64 output shape: (4932, 14, 384), dtype: float64\n",
            "Extracting tangent space features...\n",
            "Tangent space features shape: (4932, 105), dtype: float64\n",
            "Combined features shape: (4932, 119)\n",
            "PCA-reduced features shape: (4932, 55), explained variance: 0.9906\n",
            "Features saved to eeg_features.csv\n"
          ]
        }
      ],
      "source": [
        "import mne\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from pyriemann.estimation import Covariances\n",
        "from pyriemann.tangentspace import TangentSpace\n",
        "from mne.decoding import CSP\n",
        "import pandas as pd\n",
        "import gc\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "mne.set_log_level('WARNING')\n",
        "\n",
        "# Custom Transformer to Enforce float64\n",
        "class EnsureFloat64(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_out = X.astype(np.float64)\n",
        "        print(f\"EnsureFloat64 output shape: {X_out.shape}, dtype: {X_out.dtype}\")\n",
        "        return X_out\n",
        "\n",
        "# Tangent Space Features\n",
        "class TangentSpaceFeatures(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_channels=14):\n",
        "        self.n_channels = n_channels\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if X.dtype != np.float64:\n",
        "            X = X.astype(np.float64)\n",
        "\n",
        "        # Compute covariance matrices with Ledoit-Wolf estimator\n",
        "        cov_estimator = Covariances(estimator='lwf')  # Use Ledoit-Wolf with built-in shrinkage\n",
        "        cov_matrices = cov_estimator.fit_transform(X)\n",
        "\n",
        "        # Optional: Add manual diagonal loading for extra stability\n",
        "        reg = 1e-6  # Small regularization term\n",
        "        for i in range(cov_matrices.shape[0]):\n",
        "            cov_matrices[i] += np.eye(cov_matrices.shape[1]) * reg\n",
        "\n",
        "        # Project to tangent space\n",
        "        ts = TangentSpace(metric='riemann')\n",
        "        tangent_features = ts.fit_transform(cov_matrices)\n",
        "\n",
        "        print(f\"Tangent space features shape: {tangent_features.shape}, dtype: {tangent_features.dtype}\")\n",
        "        gc.collect()\n",
        "        return tangent_features\n",
        "\n",
        "def extract_features():\n",
        "    output_folder = 'processed_eegmmidb_data'\n",
        "    feature_output = 'eeg_features.csv'\n",
        "    sfreq = 128\n",
        "\n",
        "    # Load epochs\n",
        "    epoch_files = [f for f in os.listdir(output_folder) if f.endswith('-epo.fif')]\n",
        "    all_epochs = []\n",
        "    epoch_counts = []\n",
        "\n",
        "    for file in epoch_files:\n",
        "        file_path = os.path.join(output_folder, file)\n",
        "        try:\n",
        "            epochs = mne.read_epochs(file_path, preload=True, verbose=False)\n",
        "            epochs = epochs.crop(tmin=0.0, tmax=3.0, include_tmax=False)\n",
        "            epochs = epochs.resample(sfreq)\n",
        "            if len(epochs) < 10:\n",
        "                print(f\"Skipping {file}: insufficient epochs ({len(epochs)})\")\n",
        "                continue\n",
        "            print(f\"Loaded {file}: {len(epochs)} epochs\")\n",
        "            all_epochs.append(epochs)\n",
        "            epoch_counts.append(len(epochs))\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if not all_epochs:\n",
        "        raise ValueError(\"No valid epochs loaded. Check preprocessing output.\")\n",
        "\n",
        "    combined_epochs = mne.concatenate_epochs(all_epochs)\n",
        "\n",
        "    X = combined_epochs.get_data().astype(np.float64)\n",
        "    y = combined_epochs.events[:, 2]  # Labels: 0 (left_fist), 1 (both_feet)\n",
        "\n",
        "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    if np.any(np.isinf(X)):\n",
        "        raise ValueError(\"Data contains infinite values after preprocessing\")\n",
        "\n",
        "    print(f\"Loaded {X.shape[0]} epochs, {X.shape[1]} channels, {X.shape[2]} time points\")\n",
        "    print(f\"Class distribution - left_fist: {np.sum(y == 0)}, both_feet: {np.sum(y == 1)}\")\n",
        "\n",
        "    # Get channel names\n",
        "    ch_names = combined_epochs.info['ch_names']\n",
        "    expected_channels = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1',\n",
        "                        'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
        "    if len(ch_names) != 14 or ch_names != expected_channels:\n",
        "        raise ValueError(f\"Expected 14 channels {expected_channels}, got {len(ch_names)}: {ch_names}\")\n",
        "\n",
        "    # Define pipelines\n",
        "    csp_pipe = Pipeline([\n",
        "        ('ensure_float64', EnsureFloat64()),\n",
        "        ('csp', CSP(n_components=14, reg='ledoit_wolf', log=True)),  # Updated to 14 components\n",
        "        ('csp_scaler', StandardScaler(copy=True)),\n",
        "    ])\n",
        "\n",
        "    tangent_pipe = Pipeline([\n",
        "        ('tangent', TangentSpaceFeatures(n_channels=14)),\n",
        "        ('tangent_scaler', StandardScaler(copy=True)),\n",
        "    ])\n",
        "\n",
        "    # Extract features\n",
        "    print(\"Extracting CSP features...\")\n",
        "    csp_features = csp_pipe.fit_transform(X, y)\n",
        "    print(\"Extracting tangent space features...\")\n",
        "    tangent_features = tangent_pipe.fit_transform(X, y)\n",
        "\n",
        "    # Combine features\n",
        "    features = np.hstack((csp_features, tangent_features))\n",
        "    print(f\"Combined features shape: {features.shape}\")\n",
        "\n",
        "    # Apply PCA\n",
        "    pca = PCA(n_components=0.99, random_state=229)\n",
        "    features = pca.fit_transform(features)\n",
        "    print(f\"PCA-reduced features shape: {features.shape}, explained variance: {sum(pca.explained_variance_ratio_):.4f}\")\n",
        "\n",
        "    # Create feature names\n",
        "    n_csp = 14  # Updated to 14 components\n",
        "    n_tangent = tangent_features.shape[1]\n",
        "    feature_names = (\n",
        "        [f'CSP_{i}' for i in range(n_csp)] +\n",
        "        [f'Tangent_{i}' for i in range(n_tangent)]\n",
        "    )\n",
        "\n",
        "    pca_feature_names = [f'PC_{i}' for i in range(features.shape[1])]\n",
        "\n",
        "    # Save to CSV\n",
        "    feature_df = pd.DataFrame(features, columns=pca_feature_names)\n",
        "    feature_df['label'] = y\n",
        "    feature_df.to_csv(feature_output, index=False)\n",
        "    print(f\"Features saved to {feature_output}\")\n",
        "\n",
        "    # Clear memory\n",
        "    del X, y, combined_epochs, csp_features, tangent_features, features, feature_df\n",
        "    gc.collect()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    extract_features()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA5p54zgz_oy",
        "outputId": "eeb7332a-0f06-44c8-922a-6aa82d7b1490"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded features shape: (4932, 55)\n",
            "Class distribution - Left Fist: 2478, Both Feet: 2454\n",
            "Starting cross-validation...\n",
            "\n",
            "Cross-Validation Scores: [0.82674772 0.87639311 0.85801217 0.87119675 0.85598377]\n",
            "Mean CV Accuracy: 0.8577 ± 0.0173\n",
            "\n",
            "Training final model...\n",
            "\n",
            "Evaluating individual models...\n",
            "Metrics plot saved to metrics_randomforest.png\n",
            "Metrics plot saved to metrics_xgboost.png\n",
            "\n",
            "Evaluating ensemble model...\n",
            "Metrics plot saved to metrics_ensemble.png\n",
            "\n",
            "Model Performance:\n",
            "RandomForest:\n",
            "  Accuracy: 0.7903\n",
            "  Log Loss: 0.5540\n",
            "XGBoost:\n",
            "  Accuracy: 0.8369\n",
            "  Log Loss: 0.3931\n",
            "Ensemble:\n",
            "  Accuracy: 0.8470\n",
            "  Log Loss: 0.4655\n",
            "\n",
            "Metrics saved to model_metrics.csv\n",
            "Trained model saved to trained_model.joblib\n",
            "Feature importance plot saved to feature_importance_randomforest.png\n",
            "Random Forest plot saved as rf_loss_accuracy.png\n",
            "XGBoost plot saved as xgb_loss_accuracy.png\n",
            "Ensemble plot saved as ensemble_loss_accuracy.png\n",
            "Plots saved for Random Forest, XGBoost, and Ensemble models.\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "# 1. Load Features\n",
        "def load_features(feature_file):\n",
        "    df = pd.read_csv(feature_file)\n",
        "    X = df.drop('label', axis=1).values.astype(np.float64)\n",
        "    y = df['label'].values.astype(np.int32)\n",
        "    feature_names = df.drop('label', axis=1).columns.tolist()\n",
        "\n",
        "    print(f\"Loaded features shape: {X.shape}\")\n",
        "    print(f\"Class distribution - Left Fist: {np.sum(y == 0)}, Both Feet: {np.sum(y == 1)}\")\n",
        "    return X, y, feature_names\n",
        "\n",
        "# 2. Pipeline\n",
        "def create_training_pipeline():\n",
        "    return Pipeline([\n",
        "        ('scaler', StandardScaler(copy=True)),\n",
        "        ('pca', PCA(n_components=0.95, copy=True)),\n",
        "        ('ensemble', VotingClassifier(\n",
        "            estimators=[\n",
        "                ('rf', RandomForestClassifier(n_estimators=100, max_depth=7,\n",
        "                                            class_weight='balanced', random_state=42)),\n",
        "                ('xgb', XGBClassifier(n_estimators=100, learning_rate=0.05,\n",
        "                                     eval_metric='logloss', random_state=42, tree_method='hist'))\n",
        "            ],\n",
        "            voting='soft'\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "# 3. Feature Importance Visualization\n",
        "def plot_feature_importance(pipeline, X_train, feature_names, model_name):\n",
        "    rf = pipeline.named_steps['ensemble'].estimators_[0]  # RandomForest\n",
        "    importances = rf.feature_importances_\n",
        "    indices = np.argsort(importances)[::-1][:10]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(10), importances[indices], align='center')\n",
        "    plt.xticks(range(10), [feature_names[i] for i in indices], rotation=45)\n",
        "    plt.title(f'Top 10 Feature Importances ({model_name})')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'feature_importance_{model_name.lower()}.png')\n",
        "    plt.close()\n",
        "    print(f\"Feature importance plot saved to feature_importance_{model_name.lower()}.png\")\n",
        "\n",
        "# 4. Plotting Metrics for Individual Models and Ensemble\n",
        "def plot_model_metrics(pipeline, X_test, y_test, model_name, estimator=None):\n",
        "    # If estimator is provided, use it (for individual models); otherwise, use pipeline (ensemble)\n",
        "    if estimator:\n",
        "        y_pred = estimator.predict(X_test)\n",
        "        y_proba = estimator.predict_proba(X_test)\n",
        "    else:\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "        y_proba = pipeline.predict_proba(X_test)\n",
        "\n",
        "    # Compute metrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    loss = log_loss(y_test, y_proba)\n",
        "\n",
        "    # Create plots\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    plt.subplot(1, 3, 1)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    ConfusionMatrixDisplay(cm, display_labels=['Left Fist', 'Both Feet']).plot(ax=plt.gca())\n",
        "    plt.title(f'Confusion Matrix ({model_name})')\n",
        "\n",
        "    # ROC Curve\n",
        "    plt.subplot(1, 3, 2)\n",
        "    RocCurveDisplay.from_predictions(y_test, y_proba[:, 1], ax=plt.gca(), name=model_name)\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r')\n",
        "    plt.title(f'ROC Curve ({model_name})')\n",
        "\n",
        "    # Precision-Recall Curve\n",
        "    plt.subplot(1, 3, 3)\n",
        "    PrecisionRecallDisplay.from_predictions(y_test, y_proba[:, 1], ax=plt.gca(), name=model_name)\n",
        "    plt.title(f'Precision-Recall Curve ({model_name})')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'metrics_{model_name.lower()}.png')\n",
        "    plt.close()\n",
        "    print(f\"Metrics plot saved to metrics_{model_name.lower()}.png\")\n",
        "\n",
        "    return {'accuracy': acc, 'log_loss': loss}\n",
        "\n",
        "# 5. Training and Evaluation\n",
        "def train_and_evaluate(pipeline, X_train, y_train, X_test, y_test):\n",
        "    # Fit feature pipeline (scaler + PCA)\n",
        "    feature_pipeline = Pipeline(pipeline.steps[:-1])\n",
        "    X_train_transformed = feature_pipeline.fit_transform(X_train, y_train)\n",
        "    X_test_transformed = feature_pipeline.transform(X_test)\n",
        "\n",
        "    # Fit the full pipeline\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate individual models\n",
        "    rf = pipeline.named_steps['ensemble'].estimators_[0]  # RandomForest\n",
        "    xgb = pipeline.named_steps['ensemble'].estimators_[1]  # XGBoost\n",
        "\n",
        "    # Store metrics\n",
        "    metrics = {}\n",
        "\n",
        "    # Individual models (predict on transformed data)\n",
        "    print(\"\\nEvaluating individual models...\")\n",
        "    metrics['RandomForest'] = plot_model_metrics(pipeline, X_test_transformed, y_test, 'RandomForest', rf)\n",
        "    metrics['XGBoost'] = plot_model_metrics(pipeline, X_test_transformed, y_test, 'XGBoost', xgb)\n",
        "\n",
        "    # Ensemble model\n",
        "    print(\"\\nEvaluating ensemble model...\")\n",
        "    metrics['Ensemble'] = plot_model_metrics(pipeline, X_test, y_test, 'Ensemble')\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nModel Performance:\")\n",
        "    for model_name, result in metrics.items():\n",
        "        print(f\"{model_name}:\")\n",
        "        print(f\"  Accuracy: {result['accuracy']:.4f}\")\n",
        "        print(f\"  Log Loss: {result['log_loss']:.4f}\")\n",
        "\n",
        "    # Save metrics to CSV\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Model': metrics.keys(),\n",
        "        'Accuracy': [m['accuracy'] for m in metrics.values()],\n",
        "        'Log_Loss': [m['log_loss'] for m in metrics.values()]\n",
        "    })\n",
        "    metrics_df.to_csv('model_metrics.csv', index=False)\n",
        "    print(\"\\nMetrics saved to model_metrics.csv\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Random Forest Plot\n",
        "def plot_rf_loss_accuracy(rf_metrics):\n",
        "    epochs_rf = range(1, len(rf_metrics['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_rf, rf_metrics['train_loss'], label='Train Loss', color='blue')\n",
        "    plt.plot(epochs_rf, rf_metrics['test_loss'], label='Test Loss', color='orange')\n",
        "    plt.title('Random Forest Loss')\n",
        "    plt.xlabel('Trees')\n",
        "    plt.ylabel('Log Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_rf, rf_metrics['train_acc'], label='Train Accuracy', color='blue')\n",
        "    plt.plot(epochs_rf, rf_metrics['test_acc'], label='Test Accuracy', color='orange')\n",
        "    plt.title('Random Forest Accuracy')\n",
        "    plt.xlabel('Trees')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('rf_loss_accuracy.png')\n",
        "    plt.close()\n",
        "    print(\"Random Forest plot saved as rf_loss_accuracy.png\")\n",
        "\n",
        "# XGBoost Plot\n",
        "def plot_xgb_loss_accuracy(xgb_metrics):\n",
        "    epochs_xgb = range(1, len(xgb_metrics['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_xgb, xgb_metrics['train_loss'], label='Train Loss', color='blue')\n",
        "    plt.plot(epochs_xgb, xgb_metrics['test_loss'], label='Test Loss', color='orange')\n",
        "    plt.title('XGBoost Loss')\n",
        "    plt.xlabel('Boosting Rounds')\n",
        "    plt.ylabel('Log Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_xgb, xgb_metrics['train_acc'], label='Train Accuracy', color='blue')\n",
        "    plt.plot(epochs_xgb, xgb_metrics['test_acc'], label='Test Accuracy', color='orange')\n",
        "    plt.title('XGBoost Accuracy')\n",
        "    plt.xlabel('Boosting Rounds')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('xgb_loss_accuracy.png')\n",
        "    plt.close()\n",
        "    print(\"XGBoost plot saved as xgb_loss_accuracy.png\")\n",
        "\n",
        "# Ensemble Plot\n",
        "def plot_ensemble_loss_accuracy(ensemble_metrics):\n",
        "    epochs_ensemble = range(1, len(ensemble_metrics['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_ensemble, ensemble_metrics['train_loss'], label='Train Loss', color='blue')\n",
        "    plt.plot(epochs_ensemble, ensemble_metrics['test_loss'], label='Test Loss', color='orange')\n",
        "    plt.title('Ensemble Loss')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Log Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_ensemble, ensemble_metrics['train_acc'], label='Train Accuracy', color='blue')\n",
        "    plt.plot(epochs_ensemble, ensemble_metrics['test_acc'], label='Test Accuracy', color='orange')\n",
        "    plt.title('Ensemble Accuracy')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('ensemble_loss_accuracy.png')\n",
        "    plt.close()\n",
        "    print(\"Ensemble plot saved as ensemble_loss_accuracy.png\")\n",
        "\n",
        "\n",
        "# 7. Main Execution\n",
        "def main():\n",
        "    feature_file = 'eeg_features.csv'\n",
        "    model_file = 'trained_model.joblib'\n",
        "\n",
        "    # Load features\n",
        "    try:\n",
        "        X, y, feature_names = load_features(feature_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load features: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    if X.shape[0] < 50:\n",
        "        print(f\"Warning: Only {X.shape[0]} samples available. Consider more data.\")\n",
        "\n",
        "    # Create pipeline\n",
        "    pipeline = create_training_pipeline()\n",
        "\n",
        "    # Cross-validation (ensemble only)\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    try:\n",
        "        print(\"Starting cross-validation...\")\n",
        "        scores = cross_val_score(\n",
        "            pipeline, X, y,\n",
        "            cv=cv, n_jobs=1,\n",
        "            verbose=1,\n",
        "            scoring='accuracy',\n",
        "            error_score='raise'\n",
        "        )\n",
        "        print(f\"\\nCross-Validation Scores: {scores}\")\n",
        "        print(f\"Mean CV Accuracy: {np.mean(scores):.4f} ± {np.std(scores):.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCross-validation failed: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Train and evaluate\n",
        "    print(\"\\nTraining final model...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    metrics = train_and_evaluate(pipeline, X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Save the trained model\n",
        "    try:\n",
        "        joblib.dump(pipeline, model_file)\n",
        "        print(f\"Trained model saved to {model_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save model: {str(e)}\")\n",
        "\n",
        "    # Compute metrics for loss and accuracy plots\n",
        "    feature_pipeline = Pipeline(pipeline.steps[:-1])\n",
        "    X_train_transformed = feature_pipeline.fit_transform(X_train, y_train)\n",
        "    X_test_transformed = feature_pipeline.transform(X_test)\n",
        "\n",
        "    # Random Forest metrics\n",
        "    rf_metrics = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "    for n_trees in range(1, 101, 5):  # Step by 5 for efficiency\n",
        "        rf = RandomForestClassifier(n_estimators=n_trees, max_depth=7,\n",
        "                                   class_weight='balanced', random_state=42)\n",
        "        rf.fit(X_train_transformed, y_train)\n",
        "        rf_metrics['train_loss'].append(log_loss(y_train, rf.predict_proba(X_train_transformed)))\n",
        "        rf_metrics['test_loss'].append(log_loss(y_test, rf.predict_proba(X_test_transformed)))\n",
        "        rf_metrics['train_acc'].append(accuracy_score(y_train, rf.predict(X_train_transformed)))\n",
        "        rf_metrics['test_acc'].append(accuracy_score(y_test, rf.predict(X_test_transformed)))\n",
        "\n",
        "    # XGBoost metrics\n",
        "    xgb = XGBClassifier(n_estimators=100, learning_rate=0.05, eval_metric='logloss',\n",
        "                       random_state=42, tree_method='hist')\n",
        "    eval_set = [(X_train_transformed, y_train), (X_test_transformed, y_test)]\n",
        "    xgb.fit(X_train_transformed, y_train, eval_set=eval_set, verbose=False)\n",
        "    xgb_metrics = {\n",
        "        'train_loss': xgb.evals_result()['validation_0']['logloss'],\n",
        "        'test_loss': xgb.evals_result()['validation_1']['logloss'],\n",
        "        'train_acc': [],\n",
        "        'test_acc': []\n",
        "    }\n",
        "    for i in range(len(xgb_metrics['train_loss'])):\n",
        "        xgb.set_params(n_estimators=i+1)\n",
        "        xgb.fit(X_train_transformed, y_train)\n",
        "        xgb_metrics['train_acc'].append(accuracy_score(y_train, xgb.predict(X_train_transformed)))\n",
        "        xgb_metrics['test_acc'].append(accuracy_score(y_test, xgb.predict(X_test_transformed)))\n",
        "\n",
        "    # Ensemble metrics (simulate by training on increasing data subsets)\n",
        "    ensemble_metrics = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "    n_iterations = 20\n",
        "    data_sizes = np.linspace(0.1, 1.0, n_iterations)\n",
        "    for frac in data_sizes:\n",
        "        idx = np.random.choice(len(X_train_transformed), size=int(len(X_train_transformed) * frac), replace=False)\n",
        "        X_subset = X_train_transformed[idx]\n",
        "        y_subset = y_train[idx]\n",
        "        pipeline.fit(X_subset, y_subset)\n",
        "        ensemble_metrics['train_loss'].append(log_loss(y_train, pipeline.predict_proba(X_train_transformed)))\n",
        "        ensemble_metrics['test_loss'].append(log_loss(y_test, pipeline.predict_proba(X_test_transformed)))\n",
        "        ensemble_metrics['train_acc'].append(accuracy_score(y_train, pipeline.predict(X_train_transformed)))\n",
        "        ensemble_metrics['test_acc'].append(accuracy_score(y_test, pipeline.predict(X_test_transformed)))\n",
        "\n",
        "    # Plot feature importance for Random Forest\n",
        "    plot_feature_importance(pipeline, X_train, feature_names, 'RandomForest')\n",
        "    plot_rf_loss_accuracy(rf_metrics)\n",
        "    plot_xgb_loss_accuracy(xgb_metrics)\n",
        "    plot_ensemble_loss_accuracy(ensemble_metrics)\n",
        "    print(\"Plots saved for Random Forest, XGBoost, and Ensemble models.\")\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9htWyF3lBwpW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mne",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
